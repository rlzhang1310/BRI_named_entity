{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rlzhang1310/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/rlzhang1310/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/rlzhang1310/Library/Python/3.9/lib/python/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/rlzhang1310/Library/Python/3.9/lib/python/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import spacy_transformers\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = 'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices'\n",
    "my_ex1 = 'man, I HATE the BRI Projects! They stole all the funding away from mine college tuition.'\n",
    "my_ex2 = 'hey @twitteraccount3, I want to tell you that the hydroelectric dams were a big success. Thank you!!!'\n",
    "my_ex3 = \"Don't dirty Youth Pledge Day! Rejecting demonstrations with mass gatherings, they hamper efforts to overcome the Covid - 19 pandemic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(my_ex3)\n",
    "print([(X.text, X.label_) for X in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Do, 'O', ''), (n't, 'O', ''), (dirty, 'O', ''), (Youth, 'O', ''), (Pledge, 'O', ''), (Day, 'O', ''), (!, 'O', ''), (Rejecting, 'O', ''), (demonstrations, 'O', ''), (with, 'O', ''), (mass, 'O', ''), (gatherings, 'O', ''), (,, 'O', ''), (they, 'O', ''), (hamper, 'O', ''), (efforts, 'O', ''), (to, 'O', ''), (overcome, 'O', ''), (the, 'O', ''), (Covid, 'O', ''), (-, 'O', ''), (19, 'O', ''), (pandemic, 'O', '')]\n"
     ]
    }
   ],
   "source": [
    "print([(X, X.ent_iob_, X.ent_type_) for X in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dirty', 'ADJ', 'dirty'),\n",
       " ('Youth', 'PROPN', 'Youth'),\n",
       " ('Pledge', 'PROPN', 'Pledge'),\n",
       " ('Day', 'PROPN', 'Day'),\n",
       " ('Rejecting', 'VERB', 'reject'),\n",
       " ('demonstrations', 'NOUN', 'demonstration'),\n",
       " ('mass', 'ADJ', 'mass'),\n",
       " ('gatherings', 'NOUN', 'gathering'),\n",
       " ('hamper', 'VERB', 'hamper'),\n",
       " ('efforts', 'NOUN', 'effort'),\n",
       " ('overcome', 'VERB', 'overcome'),\n",
       " ('Covid', 'PROPN', 'Covid'),\n",
       " ('19', 'NUM', '19'),\n",
       " ('pandemic', 'ADJ', 'pandemic')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(x.orth_,x.pos_, x.lemma_) for x in [y \n",
    "                                      for y\n",
    "                                      in nlp(my_ex3) \n",
    "                                      if not y.is_stop and y.pos_ != 'PUNCT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Do', 'VBP'),\n",
       " (\"n't\", 'RB'),\n",
       " ('dirty', 'VB'),\n",
       " ('Youth', 'NNP'),\n",
       " ('Pledge', 'NNP'),\n",
       " ('Day', 'NNP'),\n",
       " ('!', '.'),\n",
       " ('Rejecting', 'VBG'),\n",
       " ('demonstrations', 'NNS'),\n",
       " ('with', 'IN'),\n",
       " ('mass', 'NN'),\n",
       " ('gatherings', 'NNS'),\n",
       " (',', ','),\n",
       " ('they', 'PRP'),\n",
       " ('hamper', 'VBP'),\n",
       " ('efforts', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('overcome', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('Covid', 'NNP'),\n",
       " ('-', ':'),\n",
       " ('19', 'CD'),\n",
       " ('pandemic', 'NN')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = preprocess(my_ex3)\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/rlzhang1310/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/r4/sgr1zrxx0038xwsxj6n3k8900000gn/T/ipykernel_3512/1065030357.py\", line 2, in <module>\n",
      "    model = AutoModelForTokenClassification.from_pretrained(\"cahya/bert-base-indonesian-NER\")\n",
      "  File \"/Users/rlzhang1310/Library/Python/3.9/lib/python/site-packages/transformers/utils/import_utils.py\", line 1450, in __getattribute__\n",
      "  File \"/Users/rlzhang1310/Library/Python/3.9/lib/python/site-packages/transformers/utils/import_utils.py\", line 1438, in requires_backends\n",
      "ImportError: \n",
      "AutoModelForTokenClassification requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\n",
      "installation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\n",
      "Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rlzhang1310/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/rlzhang1310/Library/Python/3.9/lib/python/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/rlzhang1310/Library/Python/3.9/lib/python/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/rlzhang1310/Library/Python/3.9/lib/python/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/rlzhang1310/Library/Python/3.9/lib/python/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/Users/rlzhang1310/Library/Python/3.9/lib/python/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/Users/rlzhang1310/Library/Python/3.9/lib/python/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/Users/rlzhang1310/Library/Python/3.9/lib/python/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/rlzhang1310/Library/Python/3.9/lib/python/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/Users/rlzhang1310/Library/Python/3.9/lib/python/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/rlzhang1310/Library/Python/3.9/lib/python/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/Users/rlzhang1310/Library/Python/3.9/lib/python/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/rlzhang1310/Library/Python/3.9/lib/python/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/Users/rlzhang1310/Library/Python/3.9/lib/python/site-packages/executing/executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"cahya/bert-base-indonesian-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"cahya/bert-base-indonesian-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def remove_unnecessary_char(text):\n",
    "    text = re.sub(\"\\[USERNAME\\]\", \" \", text)\n",
    "    text = re.sub(\"\\[URL\\]\", \" \", text)\n",
    "    text = re.sub(\"\\[SENSITIVE-NO\\]\", \" \", text)\n",
    "    text = re.sub('  +', ' ', text)\n",
    "    return text\n",
    "\n",
    "def preprocess_tweet(text):\n",
    "    text = re.sub('\\n',' ',text)\n",
    "    text = re.sub('^(\\@\\w+ ?)+',' ',text)\n",
    "    text = re.sub(r'\\@\\w+',' ',text)\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))',' ',text)\n",
    "    text = re.sub('/', ' ', text)\n",
    "    text = re.sub('  +', ' ', text)\n",
    "    return text\n",
    "\n",
    "def remove_nonaplhanumeric(text):\n",
    "    text = re.sub('[^0-9a-zA-Z]+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = preprocess_tweet(text)\n",
    "    text = remove_unnecessary_char(text)\n",
    "    text = remove_nonaplhanumeric(text)\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/rlzhang1310/Coding/buntain/ner_testing.ipynb Cell 5\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rlzhang1310/Coding/buntain/ner_testing.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m pretrained_sentiment \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mw11wo/indonesian-roberta-base-sentiment-classifier\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rlzhang1310/Coding/buntain/ner_testing.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m pretrained_ner \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcahya/bert-base-indonesian-NER\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rlzhang1310/Coding/buntain/ner_testing.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m sentiment_pipeline \u001b[39m=\u001b[39m pipeline(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rlzhang1310/Coding/buntain/ner_testing.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39msentiment-analysis\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rlzhang1310/Coding/buntain/ner_testing.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     model\u001b[39m=\u001b[39;49mpretrained_sentiment,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rlzhang1310/Coding/buntain/ner_testing.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mpretrained_sentiment,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rlzhang1310/Coding/buntain/ner_testing.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     return_all_scores\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rlzhang1310/Coding/buntain/ner_testing.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rlzhang1310/Coding/buntain/ner_testing.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m ner_pipeline \u001b[39m=\u001b[39m pipeline(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rlzhang1310/Coding/buntain/ner_testing.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mner\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rlzhang1310/Coding/buntain/ner_testing.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     model\u001b[39m=\u001b[39mpretrained_ner,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rlzhang1310/Coding/buntain/ner_testing.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     tokenizer\u001b[39m=\u001b[39mpretrained_ner,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rlzhang1310/Coding/buntain/ner_testing.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     grouped_entities\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rlzhang1310/Coding/buntain/ner_testing.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rlzhang1310/Coding/buntain/ner_testing.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m examples \u001b[39m=\u001b[39m [\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rlzhang1310/Coding/buntain/ner_testing.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mJokowi sangat kecewa dengan POLRI atas kerusuhan yang terjadi di Malang\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rlzhang1310/Coding/buntain/ner_testing.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mLesti marah terhadap perlakuan KDRT yang dilakukan oleh Bilar\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rlzhang1310/Coding/buntain/ner_testing.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mUngkapan rasa bahagia diutarakan oleh Coki Pardede karena kebabasannya dari penjara\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rlzhang1310/Coding/buntain/ner_testing.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m ]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/__init__.py:870\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m framework \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    869\u001b[0m     model_classes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[0;32m--> 870\u001b[0m     framework, model \u001b[39m=\u001b[39m infer_framework_load_model(\n\u001b[1;32m    871\u001b[0m         model,\n\u001b[1;32m    872\u001b[0m         model_classes\u001b[39m=\u001b[39;49mmodel_classes,\n\u001b[1;32m    873\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    874\u001b[0m         framework\u001b[39m=\u001b[39;49mframework,\n\u001b[1;32m    875\u001b[0m         task\u001b[39m=\u001b[39;49mtask,\n\u001b[1;32m    876\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs,\n\u001b[1;32m    877\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m    878\u001b[0m     )\n\u001b[1;32m    880\u001b[0m model_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n\u001b[1;32m    881\u001b[0m hub_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/base.py:220\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39mSelect framework (TensorFlow or PyTorch) to use from the `model` passed. Returns a tuple (framework, model).\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39m    `Tuple`: A tuple framework, model.\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tf_available() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_available():\n\u001b[0;32m--> 220\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    221\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAt least one of TensorFlow 2.0 or PyTorch should be installed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    222\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    223\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo install PyTorch, read the instructions at https://pytorch.org/.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    224\u001b[0m     )\n\u001b[1;32m    225\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    226\u001b[0m     model_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_from_pipeline\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m task\n",
      "\u001b[0;31mRuntimeError\u001b[0m: At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "pretrained_sentiment = \"w11wo/indonesian-roberta-base-sentiment-classifier\"\n",
    "pretrained_ner = \"cahya/bert-base-indonesian-NER\"\n",
    "\n",
    "sentiment_pipeline = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=pretrained_sentiment,\n",
    "    tokenizer=pretrained_sentiment,\n",
    "    return_all_scores=True\n",
    ")\n",
    "\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\",\n",
    "    model=pretrained_ner,\n",
    "    tokenizer=pretrained_ner,\n",
    "    grouped_entities=True\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    \"Jokowi sangat kecewa dengan POLRI atas kerusuhan yang terjadi di Malang\",\n",
    "    \"Lesti marah terhadap perlakuan KDRT yang dilakukan oleh Bilar\",\n",
    "    \"Ungkapan rasa bahagia diutarakan oleh Coki Pardede karena kebabasannya dari penjara\"\n",
    "]\n",
    "\n",
    "def sentiment_analysis(text):\n",
    "    output = sentiment_pipeline(text)\n",
    "    return {elm[\"label\"]: elm[\"score\"] for elm in output[0]}\n",
    "\n",
    "def ner(text):\n",
    "    output = ner_pipeline(text)\n",
    "    for elm in output:\n",
    "        elm['entity'] = elm['entity_group']\n",
    "    print(text)\n",
    "    for o in output:\n",
    "        print(o)\n",
    "    return {\"text\": text, \"entities\": output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "example1 = \"say haii dong mutss baru\"\n",
    "example2 = \"jgn kotori hari sumpah pemuda ! Tolak aksi unjuuk rasa dg n pengumpulan massa, mrk menghambat upaya penanggulangan pandemi Covid - 19\"\n",
    "example3 = \"@Balikjiheon Onejy balik ngapain ngajak aku bobo??aku gug mahu kudel2 sama kamu kan udh ada ka markeu\"\n",
    "example4 = \"leave ae wkwk tpi balik lagi bentaran mumpung libur https://t.co/1BVESJpfFH\"\n",
    "example5 = \"RT @kopikelabu: kadang, kalo interaksi sama orang yang baru dikenal. gue takut salah sikap.takutnya gue sok asik dan dia ngerasa kegangg…\"\n",
    "more_examples= [example1, example2, example3, example4, example5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jokowi sangat kecewa dengan POLRI atas kerusuhan yang terjadi di Malang\n",
      "{'entity_group': 'PER', 'score': 0.99271363, 'word': 'jokowi', 'start': 0, 'end': 6, 'entity': 'PER'}\n",
      "{'entity_group': 'NOR', 'score': 0.9850889, 'word': 'polri', 'start': 28, 'end': 33, 'entity': 'NOR'}\n",
      "{'entity_group': 'GPE', 'score': 0.98497504, 'word': 'malang', 'start': 65, 'end': 71, 'entity': 'GPE'}\n",
      "Lesti marah terhadap perlakuan KDRT yang dilakukan oleh Bilar\n",
      "{'entity_group': 'PER', 'score': 0.9636946, 'word': 'lesti', 'start': 0, 'end': 5, 'entity': 'PER'}\n",
      "{'entity_group': 'PER', 'score': 0.85859984, 'word': 'bila', 'start': 56, 'end': 60, 'entity': 'PER'}\n",
      "Ungkapan rasa bahagia diutarakan oleh Coki Pardede karena kebabasannya dari penjara\n",
      "{'entity_group': 'PER', 'score': 0.8860379, 'word': 'coki pardede', 'start': 38, 'end': 50, 'entity': 'PER'}\n"
     ]
    }
   ],
   "source": [
    "for example in examples:\n",
    "    ner(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\", use_fast=False)\n",
    "\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "\n",
    "tokenizer.src_lang = \"id_ID\"\n",
    "# ex = example[0]\n",
    "# print(ex)\n",
    "# encoded_id = tokenizer(ex, return_tensors=\"pt\")\n",
    "# generated_tokens = model.generate(\n",
    "#     **encoded_id,\n",
    "#     forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"]\n",
    "# )\n",
    "# tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leave ae wkwk tpi balik lagi bentaran mumpung libur https://t.co/1BVESJpfFH\n",
      "['leave ae wkwk tpi back to the watershed. https://t.co/1BVESJpfFH']\n"
     ]
    }
   ],
   "source": [
    "ex = more_examples[3]\n",
    "print(ex)\n",
    "encoded_id = tokenizer(ex, return_tensors=\"pt\")\n",
    "generated_tokens = model.generate(\n",
    "    **encoded_id,\n",
    "    forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"]\n",
    ")\n",
    "translated = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "print(translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leave ae wkwk tpi back to the watershed. https://t.co/1BVESJpfFH\n",
      "('ae wkwk tpi', 'ORG')\n"
     ]
    }
   ],
   "source": [
    "spacy_model = spacy.load('en_core_web_trf')\n",
    "# import en_core_web_trf\n",
    "# nlp = en_core_web_trf.load()\n",
    "# spacy_model = spacy.load('en_core_web_sm')\n",
    "print(translated[0])\n",
    "ner_text = spacy_model(translated[0])\n",
    "# ner_text = spacy_model(\"I am testing a modle right now, there is a cat and a dog and they should be detected\")\n",
    "# ner_text = nlp(translated[0])\n",
    "for word in ner_text.ents:\n",
    "    tuple = (word.text, word.label_)\n",
    "    print(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "say haii dong mutss baru\n",
      "jgn kotori hari sumpah pemuda ! Tolak aksi unjuuk rasa dg n pengumpulan massa, mrk menghambat upaya penanggulangan pandemi Covid - 19\n",
      "{'entity_group': 'EVT', 'score': 0.7844637, 'word': 'hari sumpah pemuda', 'start': 11, 'end': 29, 'entity': 'EVT'}\n",
      "{'entity_group': 'EVT', 'score': 0.51169705, 'word': 'covid -', 'start': 123, 'end': 130, 'entity': 'EVT'}\n",
      "{'entity_group': 'PRD', 'score': 0.3486499, 'word': '19', 'start': 131, 'end': 133, 'entity': 'PRD'}\n",
      "@Balikjiheon Onejy balik ngapain ngajak aku bobo??aku gug mahu kudel2 sama kamu kan udh ada ka markeu\n",
      "{'entity_group': 'PRD', 'score': 0.58981454, 'word': '@ balikjiheon onej', 'start': 0, 'end': 17, 'entity': 'PRD'}\n",
      "leave ae wkwk tpi balik lagi bentaran mumpung libur https://t.co/1BVESJpfFH\n",
      "{'entity_group': 'ORG', 'score': 0.37249494, 'word': '##w', 'start': 11, 'end': 12, 'entity': 'ORG'}\n",
      "{'entity_group': 'ORG', 'score': 0.5090847, 'word': 'tpi', 'start': 14, 'end': 17, 'entity': 'ORG'}\n",
      "{'entity_group': 'PRD', 'score': 0.67450815, 'word': ': / / t. co / 1bvesjpffh', 'start': 57, 'end': 75, 'entity': 'PRD'}\n",
      "RT @kopikelabu: kadang, kalo interaksi sama orang yang baru dikenal. gue takut salah sikap.takutnya gue sok asik dan dia ngerasa kegangg…\n",
      "{'entity_group': 'ORG', 'score': 0.78134996, 'word': 'rt @', 'start': 0, 'end': 4, 'entity': 'ORG'}\n",
      "{'entity_group': 'PRD', 'score': 0.60446286, 'word': 'kopikelabu', 'start': 4, 'end': 14, 'entity': 'PRD'}\n",
      "{'entity_group': 'PER', 'score': 0.6554062, 'word': 'gu', 'start': 69, 'end': 71, 'entity': 'PER'}\n",
      "{'entity_group': 'PER', 'score': 0.81076926, 'word': 'gu', 'start': 100, 'end': 102, 'entity': 'PER'}\n"
     ]
    }
   ],
   "source": [
    "for example in more_examples:\n",
    "    ner(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\", use_fast=False)\n",
    "\n",
    "# article_hi = \"संयुक्त राष्ट्र के प्रमुख का कहना है कि सीरिया में कोई सैन्य समाधान नहीं है\"\n",
    "# article_ar = \"الأمين العام للأمم المتحدة يقول إنه لا يوجد حل عسكري في سوريا.\"\n",
    "\n",
    "# model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "# tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "\n",
    "# # translate Hindi to French\n",
    "# tokenizer.src_lang = \"hi_IN\"\n",
    "# encoded_hi = tokenizer(article_hi, return_tensors=\"pt\")\n",
    "# generated_tokens = model.generate(\n",
    "#     **encoded_hi,\n",
    "#     forced_bos_token_id=tokenizer.lang_code_to_id[\"fr_XX\"]\n",
    "# )\n",
    "# tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "# # => \"Le chef de l 'ONU affirme qu 'il n 'y a pas de solution militaire dans la Syrie.\"\n",
    "\n",
    "# # translate Arabic to English\n",
    "# tokenizer.src_lang = \"ar_AR\"\n",
    "# encoded_ar = tokenizer(article_ar, return_tensors=\"pt\")\n",
    "# generated_tokens = model.generate(\n",
    "#     **encoded_ar,\n",
    "#     forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"]\n",
    "# )\n",
    "# tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
