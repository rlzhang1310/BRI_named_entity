{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import zipfile\n",
    "import json\n",
    "import io\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import spacy_fastlang\n",
    "import matplotlib.pyplot as plt\n",
    "from spacy.language import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_host = 'https://obj.umiacs.umd.edu'\n",
    "access_key_id = \"xxxxx\"\n",
    "secret_access_key = \"xxxxx\"\n",
    "\n",
    "s3 = boto3.client('s3', \n",
    "                  endpoint_url=s3_host, \n",
    "                  aws_access_key_id=access_key_id, \n",
    "                  aws_secret_access_key=secret_access_key)\n",
    "\n",
    "buntain = 'buntain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy_fastlang.LanguageDetector at 0x2d39e6990>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict = {'en': spacy.load('en_core_web_sm'), 'weaker': spacy.load('xx_ent_wiki_sm'), 'better':spacy.load('xx_sent_ud_sm'), \n",
    "              'zh': spacy.load('zh_core_web_sm'), 'ja': spacy.load('ja_core_news_sm'), 'es': spacy.load('es_core_news_sm'), \n",
    "              'ru': spacy.load('ru_core_news_sm'), 'fr': spacy.load('fr_core_news_sm'), 'ro': spacy.load('ro_core_news_sm'),\n",
    "              'nl': spacy.load('nl_core_news_sm'), 'pl': spacy.load('pl_core_news_sm'), 'pt': spacy.load('pt_core_news_sm'),\n",
    "              'fi': spacy.load('fi_core_news_sm'), 'de': spacy.load('de_core_news_sm'), 'ca': spacy.load('ca_core_news_sm')}\n",
    "model_dict['better'].add_pipe(\"language_detector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(folder_prefix):\n",
    "# def process():\n",
    "    entity_dict = dict()\n",
    "    response = s3.list_objects_v2(Bucket=buntain, Prefix=folder_prefix)\n",
    "    i = 0\n",
    "    count = 0\n",
    "    files = []\n",
    "    skipped = 0\n",
    "    for obj in response.get('Contents', []):\n",
    "        i += 1\n",
    "        object_key = obj['Key']\n",
    "        if object_key.endswith('.zip'):\n",
    "            files.append(object_key)\n",
    "    print(f\"number of files: {i}\")\n",
    "    for file in files:\n",
    "        zip_object = s3.get_object(Bucket=buntain, Key=file)\n",
    "        zip_contents = zip_object['Body'].read()\n",
    "        zip_file = zipfile.ZipFile(io.BytesIO(zip_contents), 'r')\n",
    "        for file_info in zip_file.infolist():\n",
    "            with zip_file.open(file_info) as json_file:\n",
    "                file_name = file_info.filename                    \n",
    "                if not file_name.endswith(\"-tweet.json\"):\n",
    "                    continue\n",
    "                try:\n",
    "                    json_data = json_file.read().decode('utf-8')\n",
    "                except: \n",
    "                    print(\"this is a text file\")\n",
    "                parsed_data = json.loads(json_data)\n",
    "                for ind_data in parsed_data:\n",
    "                    count += 1    \n",
    "                    raw_text = ind_data['tweet']['tweet_text']\n",
    "                    if 'tweet_language' not in ind_data['tweet']:\n",
    "                        NER = model_dict['better']\n",
    "                        ner_text = NER(raw_text)\n",
    "                        if ner_text._.language not in entity_dict:\n",
    "                            for word in ner_text.ents:\n",
    "                                tuple = (word.text, word.label_, lang)\n",
    "                                if tuple in entity_dict:\n",
    "                                    entity_dict[tuple]+=1\n",
    "                                else:\n",
    "                                    entity_dict[tuple]=1\n",
    "                            continue\n",
    "                        else:\n",
    "                            lang = NER._.language\n",
    "                    lang = ind_data['tweet']['tweet_language'] \n",
    "                    if lang in model_dict:\n",
    "                        NER = model_dict[lang]\n",
    "                    else:\n",
    "                        NER = model_dict['better']\n",
    "                    ner_text = NER(raw_text)\n",
    "                    if (len(ner_text.ents) == 0):\n",
    "                        skipped+=1\n",
    "                    for word in ner_text.ents:\n",
    "                        tuple = (word.text, word.label_, lang)\n",
    "                        if tuple in entity_dict:\n",
    "                            entity_dict[tuple]+=1\n",
    "                        else:\n",
    "                            entity_dict[tuple]=1\n",
    "                        # print(word.text, word.label_)\n",
    "    print(f\"how many lines: {count}\")\n",
    "    print(f\"how many skipped: {skipped}\")\n",
    "\n",
    "    return entity_dict\n",
    "    # return pd.to_datetime(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of files: 1000\n",
      "how many lines: 66288\n",
      "how many skipped: 65261\n",
      "number of files: 1000\n",
      "how many lines: 274408\n",
      "how many skipped: 105225\n",
      "number of files: 568\n",
      "how many lines: 4418374\n",
      "how many skipped: 2828901\n"
     ]
    }
   ],
   "source": [
    "folder_prefix_lst = ['twitter.tmrc/August_2022/TMRC14_APAC_1/', 'twitter.tmrc/August_2022/TMRC14_APAC_2/', 'twitter.tmrc/October_2022/TMRC15_APAC_3/']\n",
    "dict_lst = []\n",
    "for f in folder_prefix_lst:\n",
    "    dict_lst.append(process(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict to df\n",
    "# print(len(dict_lst))\n",
    "df_lst = []\n",
    "for d in dict_lst:\n",
    "    # print(keys)\n",
    "    if len(d.keys()) == 0:\n",
    "        entity_df = pd.DataFrame()\n",
    "    else:\n",
    "        keys = list(d.keys())\n",
    "        key_tuples = list(zip(*keys))  # Unpack tuples into separate lists\n",
    "\n",
    "        # Create a DataFrame\n",
    "        entity_df = pd.DataFrame({\n",
    "            'word': key_tuples[0],\n",
    "            'type': key_tuples[1],\n",
    "            'lang': key_tuples[2],\n",
    "            'count': list(d.values())\n",
    "        })\n",
    "    df_lst.append(entity_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 word      type lang  count\n",
      "85                                  #  CARDINAL   en    114\n",
      "30                         RT @ntsana       ORG   en     50\n",
      "104                              tysm    PERSON   en     27\n",
      "67                                hai       GPE   en     23\n",
      "6                                Army       ORG   en     14\n",
      "...                               ...       ...  ...    ...\n",
      "626   RT @misterj0423: Dive Into Youâœ¨       ORG   en      1\n",
      "623                          aman sii      MISC   es      1\n",
      "622    pesa ya shopping imeishia hapo    PERSON   en      1\n",
      "621                            Mutura       GPE   en      1\n",
      "1483                         the days      DATE   en      1\n",
      "\n",
      "[1484 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "if  len(df_lst[0]) > 0:\n",
    "    print(df_lst[0].sort_values(by='count', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       word      type lang  count\n",
      "165                       #  CARDINAL   en  37930\n",
      "6                  Pakistan       GPE   en  27865\n",
      "30                  Kashmir       LOC   en  25989\n",
      "43                    China       GPE   en  11739\n",
      "77                    India       GPE   en  11479\n",
      "...                     ...       ...  ...    ...\n",
      "56511               Laigroo       ORG   en      1\n",
      "56510            Rakhchikri       ORG   en      1\n",
      "56509   Khalistan Terrorist       ORG   en      1\n",
      "56508           22-Year-Old      DATE   en      1\n",
      "128324       Rajasthan BSTC       GPE   en      1\n",
      "\n",
      "[128325 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_lst[1].sort_values(by='count', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         word      type lang   count\n",
      "58                   Pakistan       GPE   en  191550\n",
      "75                          #  CARDINAL   en  167046\n",
      "73                      India       GPE   en   70518\n",
      "596                   Kashmir       LOC   en   63426\n",
      "244                    Indian      NORP   en   52916\n",
      "...                       ...       ...  ...     ...\n",
      "324401   @ApKaBhai420 @_Zehar       PER   es       1\n",
      "324400      @ApKaBhai420 Acha       ORG   en       1\n",
      "324399  @loverofrapunzel Week       ORG   en       1\n",
      "324397  @ApKaBhai420 Icecream       ORG   en       1\n",
      "710778           Nadia Sister       ORG   en       1\n",
      "\n",
      "[710779 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_lst[2].sort_values(by='count', ascending=False))\n",
    "# print(df_lst[2][\"count\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first files: 2294\n",
      "first files: 666637\n",
      "first files: 4552895\n"
     ]
    }
   ],
   "source": [
    "# number of files: 1000\n",
    "# how many lines: 66288\n",
    "# how many skipped: 31320\n",
    "# number of files: 1000\n",
    "# how many lines: 274408\n",
    "# how many skipped: 86602\n",
    "# number of files: 568\n",
    "# how many lines: 4418374\n",
    "# how many skipped: 361433\n",
    "print(f\"first files total count: {df_lst[0]['count'].sum()}\")\n",
    "print(f\"first files total count: {df_lst[1]['count'].sum()}\")\n",
    "print(f\"first files total count: {df_lst[2]['count'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "name = ['TMRC14_APAC_1', 'TMRC14_APAC_2', 'TMRC15_APAC_3']\n",
    "for time_df in df_lst:\n",
    "    time_df.to_csv(f\"~/Coding/buntain/named_entity_data/{name[i]}.csv\", index=False)\n",
    "    i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
